# Kolla-ansible 部署尝试

[TOC]

## 前言

原先的计划是在数据中心的 30 台集群上使用 `OpenStack` 搭建一个私有云平台，作为之后实验的基础设施使用。但是由于配置进度的原因，现在只有 15 台完全可用，因此目前计划为现在这 15 台上部署一个小规模的 OpenStack 平台，角色数量分别按比例缩减。因此，现在的架构为 15 台集群，其中 1 台为控制节点，14 台为计算节点。同时，14 台计算节点中，有 12 台同时为块存储节点，2 台同时为对象存储节点。

\~\~这里应该是部署架构图 :)\~\~

## 部署准备

安装 `ansible` 和 `kolla-ansible`：

```bash
$ sudo yum install -y epel-release python-pip
$ pip install -U pip
$ sudo yum install -y python-devel libffi-devel gcc openssl-devel libselinux-python
# Install ansible
$ sudo yum install -y ansible
$ pip install -U ansible
# Install kolla-ansible
$ pip install kolla-ansible==4.0.4
```

> 需要注意 `kolla-ansible` 的安装路径。若是使用全局安装，则之后需要复制的文件会在 `/usr/share/kolla-ansible` 下，若是使用 `python` 虚拟环境安装，需要复制的文件会在 `[venv_name]/share/kolla-ansible` 下。



修改所有角色的主机名，将主机修改为其角色名，重启后生效

```bash
vi /etc/hostname
```



在物理机装系统时手动建立物理机 cinder-volumes 卷组后，需要删除对应磁盘上的逻辑卷组，因此时是挂载着的，需要先卸载，具体操作为：

```bash
[root@compute01 ~]# umount /dev/cinder-volumes/dev_sdc7
[root@compute01 ~]# lvremove  /dev/cinder-volumes/dev_sdc8
```

此时需要记得修改系统  */etc/fstab* 里面逻辑卷的挂载信息，否则系统启动不起来。

controller 节点安装 net-tools（后面要用到 ifconfig 指令）

```bash
yum install net-tools
```



> 配置机器 ssh 互信

在 router 上 及其他节点上执行如下，本机生成 ssh rsa 密钥  再将其分发到远程主机，一般来说只需要部署节点和其他节点能通就行了

```bash
ssh-keygen
ssh-copy-id -i  .ssh/id_rsa.pub root@10.10.2.x
```



> 设置本地 docker registory 

```bash
docker run -d -v /opt/registry:/var/lib/registry -p 4000:5000 --restart=always --name registry registry:2
```

这里因为 openstack 占用了端口5000，和 docker 有冲突，所以这里在映射的时候改成4000

>  Openstack 官方推荐从源代码而不是二进制安装

使局域网内机器镜像一致（还可以加速pull），这里 Openstack 把原来放在 http://tarballs.openstack.org/kolla/images/centos-source-registry-ocata.tar.gz 的镜像源包给转移到 docker-hub 去了，而 docker-hub 下的包比较散， Openstack Ocata 版本的安装包链接: https://pan.baidu.com/s/1bFAAVaEbEutXbswZhp5TFw 提取码: wdi1 ，后面有的不存在的镜像，再到 docker-hub 进行下载，具体地址为https://hub.docker.com/u/kolla/

```bash
eg: docker pull kolla/centos-source-celiometer:ocata
```



将镜像 tag 并上传到本机镜像源

```bash
docker images | grep kolla | grep -v local | awk '{print $1,$2}' | while read -r image tag; do
    docker tag ${image}:${tag} localhost:4000/${image}:${tag}
    docker push localhost:4000/${image}:${tag}
done

```

修改其他几台主机的 docker daemon 进程设置为下：

```bash

```



controller 安装 openvswitch（貌似可以忽略这一步，安装一下依赖就可以，后面 neutron 会用到 openvswitch 作为网络）

> ```
> #永久关闭SELINUX
> 编辑/etc/selinux/config文件，并设置SELINUX=disabled，然后重启生效
> ```

```bash
#yum -y install make gcc openssl-devel autoconf automake rpm-build redhat-rpm-config python-devel openssl-devel kernel-devel kernel-debug-devel libtool wget
#wget http://openvswitch.org/releases/openvswitch-2.5.2.tar.gz
#mkdir -p ~/rpmbuild/SOURCES
#cp openvswitch-2.5.2.tar.gz ~/rpmbuild/SOURCES/ 
#cd ~/rpmbuild/SOURCES
#tar xvfz openvswitch-2.5.2.tar.gz

#sed 's/openvswitch-kmod, //g' openvswitch-2.5.2/rhel/openvswitch.spec > openvswitch-2.5.2/rhel/openvswitch_no_kmod.spec 
#rpmbuild -bb --nocheck openvswitch-2.5.2/rhel/openvswitch_no_kmod.spec
#yum localinstall ~/rpmbuild/RPMS/x86_64/openvswitch-2.5.2-1.x86_64.rpm
#service openvswitch restart
#systemctl enable openvswitch

```



 

编辑 `multinode` inventory 文件，分配服务器角色信息：

```ini
# These initial groups are the only groups required to be modified. The
# additional groups are for more control of the environment.
[control]
controller01    neutron_external_interface=eno1 network_interface=eno2

# The above can also be specified as follows:
#control[01:03]     ansible_user=kolla

# The network nodes are where your l3-agent and loadbalancers will run
# This can be the same as a host in the control group
[network:children]
control

[compute]
compute[01:08]  neutron_external_interface=eno1 network_interface=eno2  api_interface=eno2  storage_interface=eno2  tunnel_interface=eno2
compute[09:14]  neutron_external_interface=eth0 network_interface=eth1  api_interface=eth1  storage_interface=eth1  tunnel_interface=eth1

[monitoring]
controller01    neutron_external_interface=eno1 network_interface=eno2  api_interface=eno2      storage_interface=eno2  tunnel_interface=eno2

# When compute nodes and control nodes use different interfaces,
# you can specify "api_interface" and other interfaces like below:
#compute01 neutron_external_interface=eth0 api_interface=em1 storage_interface=em1 tunnel_interface=em1

[storage]
cinder[01:08]   neutron_external_interface=eno1 network_interface=eno2  api_interface=eno2      storage_interface=eno2  tunnel_interface=eno2
cinder[09:12]   neutron_external_interface=eth0 network_interface=eth1  api_interface=eth1      storage_interface=eth1  tunnel_interface=eth1
swift[01:02]    neutron_external_interface=eth0 network_interface=eth1  api_interface=eth1      storage_interface=eth1  tunnel_interface=eth1

[storage-cinder]
cinder[01:08]   neutron_external_interface=eno1 network_interface=eno2  api_interface=eno2      storage_interface=eno2  tunnel_interface=eno2
cinder[09:12]   neutron_external_interface=eth0 network_interface=eth1  api_interface=eth1      storage_interface=eth1  tunnel_interface=eth1

[storage-swift]
swift[01:02]    neutron_external_interface=eth0 network_interface=eth1  api_interface=eth1      storage_interface=eth1  tunnel_interface=eth1

# You can explicitly specify which hosts run each project by updating the
# groups in the sections below. Common services are grouped together.
[chrony-server:children]
haproxy

[chrony:children]
control
network
compute
storage
monitoring

[haproxy:children]
network

[mariadb:children]
control

[rabbitmq:children]
control

[mongodb:children]
control

[keystone:children]
control

[glance:children]
control

[nova:children]
control

[neutron:children]
network

[cinder:children]
control

[memcached:children]
control

[horizon:children]
control

[swift:children]
control

[heat:children]
control

[ceilometer:children]
control

[aodh:children]
control

[panko:children]
control

[gnocchi:children]
control

[designate:children]
control

[placement:children]
control

# Additional control implemented here. These groups allow you to control which
# services run on which hosts at a per-service level.
#
# Word of caution: Some services are required to run on the same host to
# function appropriately. For example, neutron-metadata-agent must run on the
# same host as the l3-agent and (depending on configuration) the dhcp-agent.

# Glance
[glance-api:children]
glance

[glance-registry:children]
glance

# Nova
[nova-api:children]
nova

[nova-conductor:children]
nova

[nova-consoleauth:children]
nova

[nova-novncproxy:children]
nova

[nova-scheduler:children]
nova

[nova-spicehtml5proxy:children]
nova

[nova-compute-ironic:children]
nova

[nova-serialproxy:children]
nova

# Neutron
[neutron-server:children]
control

[neutron-dhcp-agent:children]
neutron

[neutron-l3-agent:children]
neutron

[neutron-lbaas-agent:children]
neutron

[neutron-metadata-agent:children]
neutron

[neutron-vpnaas-agent:children]
neutron

# Cinder
[cinder-api:children]
cinder

[cinder-backup:children]
storage-cinder

[cinder-scheduler:children]
cinder

[cinder-volume:children]
storage-cinder

# iSCSI
[iscsid:children]
compute
storage

[tgtd:children]
storage

# Swift
[swift-proxy-server:children]
swift

[swift-account-server:children]
storage-swift

[swift-container-server:children]
storage-swift

[swift-object-server:children]
storage-swift

# Heat
[heat-api:children]
heat

[heat-api-cfn:children]
heat

[heat-engine:children]
heat

# Ceilometer
[ceilometer-api:children]
ceilometer

[ceilometer-central:children]
ceilometer

[ceilometer-notification:children]
ceilometer

[ceilometer-collector:children]
ceilometer

[ceilometer-compute:children]
compute

# Aodh
[aodh-api:children]
aodh

[aodh-evaluator:children]
aodh

[aodh-listener:children]
aodh

[aodh-notifier:children]
aodh

# Panko
[panko-api:children]
panko

# Gnocchi
[gnocchi-api:children]
gnocchi

[gnocchi-statsd:children]
gnocchi

[gnocchi-metricd:children]
gnocchi

# Designate
[designate-api:children]
designate

[designate-central:children]
designate

[designate-mdns:children]
designate

[designate-worker:children]
designate

[designate-sink:children]
designate

[designate-backend-bind9:children]
designate

# Placement
[placement-api:children]
placement
```

`Kolla` 配置文件 `/etc/kolla/global.yml` ，完成 `kolla-ansible` 配置：

```yaml
kolla_base_distro: "centos"
kolla_install_type: "source"
openstack_release: "ocata"
node_custom_config: "/etc/kolla/config"
kolla_internal_vip_address: "10.0.0.254"
docker_registry: "10.0.0.41:4000"

enable_aodh: "yes"
enable_ceilometer: "yes"
enable_chrony: "yes"
enable_cinder: "yes"
enable_cinder_backend_lvm: "yes"
enable_designate: "yes"
enable_gnocchi: "yes"
enable_heat: "yes"
enable_horizon: "yes"
enable_mongodb: "yes"
enable_panko: "yes"
enable_swift: "no"

glance_backend_file: "yes"
glance_backend_ceph: "no"

ceilometer_database_type: "mongodb"
ceilometer_event_type: "mongodb"

designate_backend: "bind9"
designate_ns_record: "dc.openstack.triplez.cn"
```

## 部署预检查

输入以下命令进行 `kolla-ansible` 的部署预检查：

```bash
$ kolla-ansible prechecks -i multinode
```

> 若使用 `virtualenv` 安装的 `kolla-ansible` ，则需要先启动虚拟环境。



> 预检查中 HAProxy 错误：
> 
> 错误信息：**TypeError: load\_config() got an unexpected keyword argument 'config\_dict'**
> 
> 错误原因：Python Docker 和 docker-py 冲突问题。
> 
> 解决方案：将两者卸载，重新安装 Python Docker，`pip uninstall docker docker-py -y && pip install docker -y` 。





> 预检查 Cinder LVM 卷组错误：
> 
> 错误信息：**Volume group \"cinder-volumes\" not found\n  Cannot process volume group cinder-volumes**
> 
> 错误原因：存储节点中缺少名为 `cinder-volumes` 的卷组
> 
> 解决方案：在存储节点中新建名为 `cinder-volumes` 的卷组。



> 预检查 Nova libvirt 服务错误：
> 
> 错误信息：**nova : Checking that libvirt is not running**
> 
> 错误原因：计算节点中启动了 `libvirt` 服务。
> 
> 解决方案：在出错的计算节点中关闭 `libvirt` 服务。



> 预检查 Neutron 服务错误：
> 
> 错误信息：**Number of network agents are less than two when enabling agent ha**
> 
> 错误原因：开启了 `enable_neutron_agent_ha` 而网络节点的数目小于 2。
> 
> 解决方案：关闭 `enable_neutron_agent_ha` 。



> 预检查 docker 服务错误：
> 
> 错误信息：**neutron : Checking if 'MountFlags' for docker service is set to 'shared'**
> 
> 错误原因：网络节点中 `docker` 没有设置分享 `MountFlags` 选项。
> 
> 解决方案：
> 
> 新建 `/etc/systemd/system/docker.service.d/kolla.conf` 并填入：
> 
> ```ini
> [Service]
> MountFlags=shared
> ```
> 
> 然后重启 `docker daemon` ：
> 
> ```bash
> $ sudo systemctl daemon-reload
> $ sudo systemctl restart docker
> ```



> 预检查 Horizon 服务错误：
> 
> 错误信息：Timeout when waiting for 10.0.0.41:80 to stop.
> 
> 错误原因：80 端口被占用。
> 
> 解决方案：关闭占用 80 端口的程序，重新运行 prechecks 。



## 部署 OpenStack

输入以下命令通过 `kolla-ansible` 来部署 `OpenStack` ：

```bash
`$ kolla-ansible deploy -i multinode
```

The requested image does not exist.

APIError: 500 Server Error: Internal Server Error (\"Get https://10.0.0.41:4000/v1/\_ping: http: server gave HTTP response to HTTPS client

更改对应主机的 `docker` 源设置。



APIError: 409 Client Error: Conflict (\"Conflict. The container name \"/fluentd\" is already in use by container d6cce705e2519a403241b5098d207c4e944746c3080c3074d686273bd772e325. You have to remove (or rename) that container to be able to reuse that name.

更改 `inventory` 文件内容，`storage` 下的机器和 `compute` 下的机器不能重复。



cp -r -f share/kolla-ansible/\* .

kolla-ansible 找不到文件。



ERROR! Unexpected Exception, this is probably a bug: No module named oslo\_config

缺少 `oslo.config` 模块。

pip install oslo.config



[haproxy: Waiting for virtual IP to appear] Timeout when waiting for 10.0.0.254:3306

部署过一次失败后，neutron 的容器服务还存留在集群中，因此网络仍然是 down 的状态。需要销毁所有容器重新部署 OpenStack。

在部署 neutron 网络服务的过程中，负责 external network 的网卡将不可用，因此不要在 SSH 外网环境下执行部署，



部署完后 Provider Network 掉线。

FAILED - RETRYING: Creating the Swift service and endpoint (10 retries left).


failed: [swift01]() (item={u'interface': u'admin', u'url': u'http://10.0.0.254:8080/v1'}) =\> {"attempts": 10, "changed": true, "item": {"interface": "admin", "url": "http://10.0.0.254:8080/v1"}, "msg": "'Traceback (most recent call last):\\n  File \"/tmp/ansible_DJpOUy/ansible_module_kolla_keystone_service.py\", line 55, in main\\n    for _service in cloud.keystone_client.services.list():\\n  File \"/opt/ansible/lib/python2.7/site-packages/shade/openstackcloud.py\", line 295, in keystone_client\\n    \\'identity\\', keystoneclient.client.Client)\\n  File \"/opt/ansible/lib/python2.7/site-packages/shade/openstackcloud.py\", line 261, in _get_client\\n    **kwargs)\\n  File \"/opt/ansible/lib/python2.7/site-packages/os_client_config/cloud_config.py\", line 355, in get_legacy_client\\n    return client_class(**constructor_kwargs)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneclient/client.py\", line 62, in Client\\n    d = discover.Discover(session=session, **kwargs)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneclient/discover.py\", line 178, in __init__\\n    authenticated=authenticated)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneclient/_discover.py\", line 143, in __init__\\n    authenticated=authenticated)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneclient/_discover.py\", line 38, in get_version_data\\n    resp = session.get(url, headers=headers, authenticated=authenticated)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/session.py\", line 1011, in get\\n    return self.request(url, \\'GET\\', **kwargs)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/session.py\", line 684, in request\\n    auth_headers = self.get_auth_headers(auth)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/session.py\", line 1071, in get_auth_headers\\n    return auth.get_headers(self, **kwargs)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/plugin.py\", line 95, in get_headers\\n    token = self.get_token(session)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/identity/base.py\", line 88, in get_token\\n    return self.get_access(session).auth_token\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/identity/base.py\", line 134, in get_access\\n    self.auth_ref = self.get_auth_ref(session)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/identity/generic/base.py\", line 206, in get_auth_ref\\n    self._plugin = self._do_create_plugin(session)\\n  File \"/opt/ansible/lib/python2.7/site-packages/keystoneauth1/identity/generic/base.py\", line 161, in _do_create_plugin\\n    \\'auth_url is correct. %s\\' % e)\\nDiscoveryFailure: Could not find versioned identity endpoints when attempting to authenticate. Please check that your auth_url is correct. Unable to establish connection to ********: HTTPConnectionPool(host=\\'10.0.0.254\\', port=35357): Max retries exceeded with url: / (Caused by NewConnectionError(\\'\<urllib3.connection.HTTPConnection object at 0x7f881f300dd0\>: Failed to establish a new connection: [Errno 113]() No route to host\\',))\\n'"}_



>  fatal: [compute01 -\> controller01](): FAILED! =\> {"attempts": 20, "changed": false, "cmd": ["docker", "exec", "kolla_toolbox", "openstack", "--os-interface", "internal", "--os-auth-url", "http://10.0.0.254:35357/v3", "--os-identity-api-version", "3", "--os-project-domain-name", "default", "--os-tenant-name", "admin", "--os-username", "admin", "--os-password", "openstackdc", "--os-user-domain-name", "default", "compute", "service", "list", "-f", "json", "--service", "nova-compute"](), "delta": "0:00:01.273804", "end": "2018-09-03 22:49:38.514252", "rc": 0, "start": "2018-09-03 22:49:37.240448", "stderr": "", "stderr_lines": [](), "stdout": "[]()", "stdout_lines": ["\[]()"]}

> RUNNING HANDLER [memcached : Restart memcached container] **********************
> fatal: [controller01]: FAILED! => {"changed": true, "msg": "'Traceback (most recent call last):\\n  File \"/tmp/ansible_kolla_docker_payload_isqJid/__main__.py\", line 801, in main\\n    result = bool(getattr(dw, module.params.get(\\'action\\'))())\\n  File \"/tmp/ansible_kolla_docker_payload_isqJid/__main__.py\", line 610, in start_container\\n    self.pull_image()\\n  File \"/tmp/ansible_kolla_docker_payload_isqJid/__main__.py\", line 452, in pull_image\\n    repository=image, tag=tag, stream=True\\n  File \"/usr/lib/python2.7/site-packages/docker/api/image.py\", line 400, in pull\\n    self._raise_for_status(response)\\n  File \"/usr/lib/python2.7/site-packages/docker/api/client.py\", line 263, in _raise_for_status\\n    raise create_api_error_from_http_exception(e)\\n  File \"/usr/lib/python2.7/site-packages/docker/errors.py\", line 31, in create_api_error_from_http_exception\\n    raise cls(e, response=response, explanation=explanation)\\nNotFound: 404 Client Error: Not Found (\"manifest for 10.10.2.30:4000/lokolla/centos-source-memcached:ocata not found\")\\n'"}

防火墙和 SELinux 没关…… 写了个脚本批量关闭。（特别是 Controller 的防火墙一定要关。



> TASK [mariadb : include] *******************************************************
> fatal: [controller01]: FAILED! => {"reason": "'always_run' is not a valid attribute for a Task\n\nThe error appears to have been in '/usr/share/kolla-ansible/ansible/roles/mariadb/tasks/lookup_cluster.yml': line 2, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n---\n- name: Cleaning up temp file on localhost\n  ^ here\n\nThis error can be suppressed as a warning using the \"invalid_task_attribute_failed\" configuration"}

查阅文档得知 alway_run 这个状态在 ansible2.6 中被替换掉了，找到预检查/部署过程中的 playbook 脚本，修改对应字段为 check_mode: no <https://docs.ansible.com/ansible/2.6/porting_guides/porting_guide_2.6.html> 



> TASK [keystone : Creating keystone database] ***********************************
> fatal: [controller01 -> controller01]: FAILED! => {"changed": false, "msg": "unable to find /var/lib/ansible/.my.cnf. Exception message: (2003, 'Can\\'t connect to MySQL server on \\'10.10.2.254\\' (110 \"Connection timed out\")')"} 

multinode 组件设置问题，忽略了部分 haproxy 组件





TASK [mariadb : include] ***********************************************************************************************************************
fatal: [controller01]: FAILED! => {"msg": "The conditional check 'delegate_host == 'None'' failed. The error was: error while evaluating conditional (delegate_host == 'None'): 'delegate_host' is undefined\n\nThe error appears to have been in '/root/Kolla-ansible/kolla-ansible/ansible/roles/mariadb/tasks/bootstrap.yml': line 4, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- include: bootstrap_cluster.yml\n  ^ here\n"}

这啥玩意啊……



 FAILED! => {"msg": "The conditional check 'remove_container.changed' failed. The error was: error while evaluating conditional (remove_container.changed): 'remove_container' is undefined\n\nThe error appears to have been in '/root/Kolla-ansible/kolla-ansible/ansible/roles/common/tasks/reconfigure.yml': line 32, column 3, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n\n- include: start.yml\n  ^ here\n"}



Swift:

Account HEAD failed: http://211.65.102.70:8080/v1/AUTH_13d7aa9f8b974db891d59ad2c192e34f 503 Service Unavailable
Failed Transaction ID: tx0c3b3c6203bd403582522-005b8fe3a8

生成 `rings` 时的 `STORAGE_NODES` 的 IP 应该是存储网络中的 IP。Swift 容器只监听存储网络 IP 中的指定端口（6000, 6001 和 6002）。



校园网内无法访问 211.65.102.70 高可用 VIP：

给 `Controller` 节点的 `br-ex` 端口上绑定一个公网 IP，并要**写好网关**，一定要写！否则路由不知道存在该 IP，路由外的节点自然无法访问。

``` bash
$ ip addr add 211.65.102.41/24 dev br-ex
$ ip route add default via 211.65.102.1
```



部署完成后外部高可用 VIP 没有绑定成功，br-ex 绑定在了 neutron 网卡上却无法 ping 通网关，此时 br-ex  没有发挥它应有的作用，使能操作如下：

```bash
ifconfig br-ex promisc up
ifconfig enp3s0 0.0.0.0
ifconfig enp3s0 promisc
ifconfig br-ex 10.10.1.10 netmask 255.255.255.0

ovs-vsctl add-port br-ex enp3s0

# Then, re-add the default route if needed
ip route add default via 10.10.1.200
```



MariaDB:

The conditional check 'delegate_host == 'None'' failed. The error was: error while evaluating conditional (delegate_host == 'None'): 'delegate_host' is undefined



重新不使用 `--tags` 部署就没这问题…… 玄学……





## 列出所有的容器 ID

```
docker ps -aq
```

## 停止所有的容器

```
docker stop $(docker ps -aq)
```

## 删除所有的容器

```
docker rm $(docker ps -aq)
```

## 删除所有的镜像

```
docker rmi $(docker images -q)
```



创建 provider 网络

```bash
neutron net-create --shared --provider:physical_network physnet1   --provider:network_type flat provider
```



```
neutron subnet-create --name provider \
  --allocation-pool start=10.10.1.69,end=10.10.1.81 \
  --dns-nameserver 8.8.8.8 --gateway 10.10.1.200 \
  provider 10.10.1.0/24
```

建立子网
```bash
neutron net-create selfservice
```
```bash
neutron subnet-create --name selfservice \
  --dns-nameserver  8.8.8.8 --gateway 172.16.1.1\
  selfservice 172.16.1.0/24
```

```
openstack server create --flavor m1.tiny --image cirros \
  --nic net-id=8f43e115-1ca2-4174-9c00-813ddb24bc2e --security-group default \
  --key-name doc_mykey selfservice-instance
  
```
建立路由器
```bash
. admin-openrc.sh
neutron net-update provider --router:external
neutron router-create router
neutron router-interface-add router selfservice
neutron router-gateway-set router provider
```


密钥对建立：
```bash
ssh-keygen -q -N ""
openstack keypair create --public-key ~/.ssh/id_rsa.pub mykey
```
请求认证令牌：

```
openstack token issue
```

管理员要求生成一个 token 令牌

```bash
openstack --os-auth-url http://10.10.2.254:35357/v3 \
  --os-project-domain-name default --os-user-domain-name default \
  --os-project-name admin --os-username admin token issue
```
安全组建立并放行 tcp 22 端口（ssh），更多的可以在 dashboard 下面去写，更方便，此处演示一下而已
```bash
openstack security group rule create --proto icmp default
openstack security group rule create --proto tcp --dst-port 22 default
```
用户要求生成 token 令牌

```bash
openstack --os-auth-url http://controller:5000/v3 \
  --os-project-domain-name default --os-user-domain-name default \
  --os-project-name demo --os-username demo token issue
```

使用如下命令请求认证token：



```bash
 curl -i 'http://10.10.2.254:5000/v2.0/tokens' -X POST -H "Content-Type: application/json" -H "Accept: application/json"  -d '{"auth": {"tenantName": "admin",
"passwordCredentials": {"username": "admin", "password": "pensees@ai.com##123"}}}'
```



## GPU虚拟化记录
---------
#### 机器设置
> 先进 bios 打开 VT-d (intel)  然后设置 Advanced 下 ROM 中 Other PCI devices 为 legacy  

下面直接复制了别人博客的内容：

> a. 确认内核支持iommu
>
> cat /proc/cmdline | grep iommu
>
> 如果没有输出结果，添加intel_iommu=on到grub的启动参数
>
> 如果你想在系统启动时加载一个内核参数，需修改GRUB的配置模板(/etc/default /grub),添加"名称=值”的键值对到GRUB_CMDLINE_LINUX变量,添加多个时用空格隔开,例如GRUB_CMDLINE_LINUX="...... name=value"(如果没有GRUB_CMDLINE_LINUX变量时,用GRUB_CMDLINE_LINUX_DEFAULT替代即可)，然后使用grub2-mkconfig -o /boot/grub2/grub.cfg更新grub.cfg文件
>
> 具体修改后的文件例如：GRUB_CMDLINE_LINUX_DEFAULT="iommu=pt intel_iommu=on"
>
> 
>
> b. 使用dmesg | grep -e DMAR -e IOMMO来判断 (DMAR与IOMMO都必须有)。
> 使用命令  ```bash lspci -nn | grep NVIDIA  ```来查看显卡信息
> 解绑显卡  ```bash virsh nodedev-list | grep pci  ``` 查询设备ID  其中pci值为刚看到的显卡打头的数字
>
> ```bash virsh nodedev-dumpxml pci_0000_85_00_0 ```      #查询当前使用的驱动程序
>
> ```bash virsh nodedev-detach pci_0000_85_00_0  ```     #解绑当前设备
>
> ```bash virsh nodedev-detach pci_0000_85_00_1  ```  #解绑audio
>
> 参考来源：https://pve.proxmox.com/wiki/Pci_passthrough     https://blog.csdn.net/fan_lulu/article/details/84258780   https://blog.csdn.net/hangdongzhang/article/details/77745557   https://blog.csdn.net/u011846257/article/details/52350369



#### 节点设置

> 参考来源：<https://blog.csdn.net/u011846257/article/details/52350369>     <https://docs.openstack.org/nova/pike/configuration/config.html>      <https://docs.openstack.org/nova/pike/admin/pci-passthrough.html>     





85:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP108 [GeForce GT 1030] [10de:1d01] (rev a1)
85:00.1 Audio device [0403]: NVIDIA Corporation GP108 High Definition Audio Controller [10de:0fb8] (rev a1)



[pci]
alias = { "vendor_id":"10de", "product_id":"1d01", "device_type":"type-PCI", "name":"NVIDIA1030" }









#### 带 GPU 显卡机器准备：
1、确认处理器支持中断重映射，查到过 nvidia 奔腾x5系列以后、酷睿 i3 以后都支持虚拟化，但找不到具体信息了，一般显卡都支持的 VT-d ，支持一对一使用，只有 支持 VT-g 的能够实现一对多使用。

要拥有单独的iommu，您的处理器需要支持称为ACS（访问控制服务）的功能。

所有Xeon处理器都支持它们（E3，E5），不包括Xeon E3-1200

对于intel Core，它是不同的，只有一些处理器支持ACS

2、确认显卡支持透传，检查 gpu rom的完整性（<https://github.com/awilliam/rom-parser>）

3、修改 PCI 设备 BIOS 面板中 ROM 引导方式 legecy 为 uefi（后面虚机的引导方式）

4、检查VT-d是否工作（BIOS中 iommu 相关参数开启，dmesg | grep -e DMAR -e IOMMU 有值为正确）
5、验证IOMMU隔离性，修改 /etc/default/grub 文件，对于 Intel 芯片:

```bash
GRUB_CMDLINE_LINUX 或 GRUB_CMDLINE_LINUX_DEFAULT 内添加 "inter_iommu = on"
```



对于AMD芯片：

```
GRUB_CMDLINE_LINUX_DEFAULT="iommu=pt iommu=1"
```

。
6、修改内核参数

```bash
vi /etc/modules
添加：
pci_stub
vfio
vfio_iommu_type1
vfio_pci
kvm
kvm_intel
```



7、禁用显卡的默认驱动（使用VFIO方式进行透传，需在内核启动项中加入）

```ba
vi /etc/modprobe.d/blacklist.conf
添加如下：
blacklist snd_hda_intel
blacklist amd76x_edac
blacklist vga16fb
blacklist nouveau
blacklist rivafb
blacklist nvidiafb
blacklist rivatv
```

保存后退出，使内核识别 vfio，执行：

```bsh
modprobe vfio 
lspci -nnk -d 10de:1d01 #最后查看driver in use 是否为 vfio-pci，是就对了
```

重启机器





#### Openstack 配置：

1、 配置nova-scheduler 调度器、nova-api 资源信息规则（控制器），查找controller 上的所有路径不含 logrotate.d 的nova.conf 文件，并修改如下：

> find / -name nova.conf

在[default]组中添加规则:

```bash
[default]
scheduler_default_filters = RetryFilter, AvailabilityZoneFilter, RamFilter, ComputeFilter, ComputeCapabilitiesFilter, ImagePropertiesFilter, ServerGroupAntiAffinityFilter, ServerGroupAffinityFilter, PciPassthroughFilter
scheduler_available_filters = nova.scheduler.filters.all_filters
```

新建[pci]组，其中name自定，product_id 为产品 id ，使用指令 lspci -nn | grep NVIDIA 查找，vendor_id 为标识 id ，10de 即代表 NVIDIA，device_type 为类型可填 type-PCI、type-PF、type-VF，具体设置可以在官方文档 <https://docs.openstack.org/ocata/config-reference/compute/config-options.html >里查到：

```bsh
[pci]
alias = { "name": "nvidia1030", "product_id": "1d01", "vendor_id": "10de", "device_type": "type-PCI" }
```

2、同上，查找并修改 GPU 显卡所在 compute 节点的 nova.conf 信息，不用修改调度器、过滤器规则，只需要新建 [pci]组，比 controller 节点多一个 passthrough_whitelist 添加本机资源信息即可。

```bash
[pci]
passthrough_whitelist = { "vendor_id": "10de", "product_id": "1d01"}
alias = {"vendor_id": "10de", "product_id": "1d01", "device_type": "type-PCI", "name": "nvidia1030"}
```

 3、 配置一个实例类型，建立带GPU的实例类型，实例类型绑定显卡，建立对应虚拟机（VT-d ，只支持一对一，如果有多个同类型显卡，可在"pci_passthrough:alias"="NVIDIA1030:2"中以数字进行标识）

```bash
openstack flavor create --public --ram 2048 --disk 20 --vcpus 2 GPU.small
```


```
openstack flavor set GPU.small --property "pci_passthrough:alias"="nvidia1030:1"
```



```
openstack server create --flavor GPU.small --image centos7 --wait nvi1030-test
```

```bash
passthrough_whitelist = { "vendor_id": "10de", "product_id": "1b81"}
passthrough_whitelist = { "vendor_id": "10de", "product_id": "1e04"}
passthrough_whitelist = { "vendor_id": "10de", "product_id": "1c82"}
alias = {"vendor_id": "10de", "product_id": "1e04", "device_type": "type-PCI", "name": "nvidia2080Ti"}
alias = {"vendor_id": "10de", "product_id": "1b81", "device_type": "type-PCI", "name": "nvidia1070"}
alias = {"vendor_id": "10de", "product_id": "1c82", "device_type": "type-PCI", "name": "nvidia1050Ti"}

```



新的实例如果在孵化时报错（如果在调度之前报错很可能在控制部分(包含了调度代码)代码出错，如果在孵化中卡着，则很可能是计算节点compute代码出错）

 4、Nvidia的驱动中对Geforce的显卡做了检查，消费级显卡不允许在虚拟机中运行，只允许专业卡比如M60进行虚拟化，所以我们将Geforce显卡做了直通的时候，驱动就会自己检查报错停止工作。著名的 error 43。Openstack 在 Pike 版本开始，才在 metedata 里多了 img_hide_hypervisor_id 属性，本文 openstack 版本为 Ocata ，无法使用，而 kolla-ansible 部署方式又注定我们无法修改源代码。这里在建立具有 GPU 的物理机后，通过使用 VFIO 来隐藏虚拟机属性。

> 参考来源：<http://vfio.blogspot.com/2014/08/>  

> ```bash
> virsh list --all #查看该 compute 节点上的所有虚拟机
> virsh edit ID    #ID为上面建立的带 GPU 的实例的 ID，编辑修改该文件里的隐藏KVM属性
> ```
>
> ```bash
> <domain type='kvm'>
>   <features>
>     <kvm>
>       <hidden state='on'/>
>     </kvm>
>   </features>
> </domain>
> ```
>
> 而后 
>
> ```bash
> virsh shutdown ID   #必须先关掉再开启，我还修改了对应的 instance-000000a.xml（因实例而异）文件
> virsh start ID 
> ```




 这时再次在虚拟机中使用 cpuid | grep hypervisor_id 指令，如果返回值为 kvm 则隐藏失败，如果结果为 @ ，则为成功。



**错误：** 实例 "8GPU_2080Ti_1" 执行所请求操作失败，实例处于错误状态。: 请稍后再试 [错误: Build of instance 38b7836b-dfdf-48bc-b9d7-6a617afcf844 aborted: VolumeSizeExceedsAvailableQuota: Requested volume or snapshot exceeds allowed gigabytes quota. Requested 1000G, quota is 1000G and 108G has been consumed.].



**错误：** 实例 "2080Ti_1" 执行所请求操作失败，实例处于错误状态。: 请稍后再试 [错误: No valid host was found. There are not enough hosts available.].



================/etc/kolla/cron/logrotate/nova.conf
/etc/kolla/nova-compute/nova.conf
/root/nova.conf
====================/var/lib/docker/overlay2/38951b0e5cb39f2af6c382d1d3c0764dffd12d4817f7e5c53614fd0b65dd186c/diff/etc/logrotate.d/nova.conf
=======================/var/lib/docker/overlay2/38951b0e5cb39f2af6c382d1d3c0764dffd12d4817f7e5c53614fd0b65dd186c/merged/etc/logrotate.d/nova.conf
/var/lib/docker/overlay2/8a20f7459128ee2eada22d6d9659df1ade8a9a82d71495c2e12c6dedffbfe712/diff/etc/nova/nova.conf
/var/lib/docker/overlay2/8a20f7459128ee2eada22d6d9659df1ade8a9a82d71495c2e12c6dedffbfe712/merged/etc/nova/nova.conf



01:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU102 [**GeForce** RTX 2080 Ti] [10de:1e04] (rev a1)
01:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:10f7] (rev a1)
01:00.2 USB controller [0c03]: NVIDIA Corporation Device [10de:1ad6] (rev a1)
01:00.3 Serial bus controller [0c80]: NVIDIA Corporation Device [10de:1ad7] (rev a1)
04:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU102 [**GeForce** RTX 2080 Ti] [10de:1e04] (rev a1)
04:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:10f7] (rev a1)
04:00.2 USB controller [0c03]: NVIDIA Corporation Device [10de:1ad6] (rev a1)
04:00.3 Serial bus controller [0c80]: NVIDIA Corporation Device [10de:1ad7] (rev a1)
08:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP104 [**GeForce GTX 1070]** [10de:1b81] (rev a1)
08:00.1 Audio device [0403]: NVIDIA Corporation GP104 High Definition Audio Controller [10de:10f0] (rev a1)
86:00.0 VGA compatible controller [0300]: NVIDIA Corporation TU102 **[GeForce RTX 2080 Ti**] [10de:1e04] (rev a1)
86:00.1 Audio device [0403]: NVIDIA Corporation Device [10de:10f7] (rev a1)
86:00.2 USB controller [0c03]: NVIDIA Corporation Device [10de:1ad6] (rev a1)
86:00.3 Serial bus controller [0c80]: NVIDIA Corporation Device [10de:1ad7] (rev a1)
8a:00.0 VGA compatible controller [0300]: NVIDIA Corporation GP107 [GeForce GTX **1050** Ti] [10de:1c82] (rev a1)
8a:00.1 Audio device [0403]: NVIDIA Corporation GP107GL High Definition Audio Controller [10de:0fb9] (rev a1)





=================/etc/kolla/cron/logrotate/nova.conf
/etc/kolla/nova-conductor/nova.conf
/etc/kolla/nova-api/nova.conf
/etc/kolla/nova-scheduler/nova.conf
/etc/kolla/nova-novncproxy/nova.conf
/etc/kolla/placement-api/nova.conf
/etc/kolla/nova-consoleauth/nova.conf
/root/nova.conf
===============/var/lib/docker/overlay2/ae357ede480c7c10687f673b9cdedb8257a8d83418e94cca06776aeb47a9ebdf/diff/etc/logrotate.d/nova.conf
=====================/var/lib/docker/overlay2/ae357ede480c7c10687f673b9cdedb8257a8d83418e94cca06776aeb47a9ebdf/merged/etc/logrotate.d/nova.conf
/var/lib/docker/overlay2/f277d241218f225a3576d92f63a3170d60c13258daa2a51a915fbace31dcc3af/diff/etc/nova/nova.conf
/var/lib/docker/overlay2/f277d241218f225a3576d92f63a3170d60c13258daa2a51a915fbace31dcc3af/merged/etc/nova/nova.conf
/var/lib/docker/overlay2/bbeffea05ad32b282c7e9d447075608cedb43dad9afcd90c61474192bf7c6608/diff/etc/nova/nova.conf
/var/lib/docker/overlay2/bbeffea05ad32b282c7e9d447075608cedb43dad9afcd90c61474192bf7c6608/merged/etc/nova/nova.conf
/var/lib/docker/overlay2/96ee59705da73c47c800e1f33c8598ba9bb27a0e15e55f1286fb3d17fce8d9cb/diff/etc/nova/nova.conf
/var/lib/docker/overlay2/96ee59705da73c47c800e1f33c8598ba9bb27a0e15e55f1286fb3d17fce8d9cb/merged/etc/nova/nova.conf
/var/lib/docker/overlay2/fe9ad7ea79894700d18df774b03d14ec3e17419fb10cec44d5056baf0f0fc7d3/diff/etc/nova/nova.conf
/var/lib/docker/overlay2/fe9ad7ea79894700d18df774b03d14ec3e17419fb10cec44d5056baf0f0fc7d3/merged/etc/nova/nova.conf
/var/lib/docker/overlay2/0ce3a0b1ec0fa5d2c7e300b004f1eecf1a28db187751c60e179c96c17ae3d5a6/diff/etc/nova/nova.conf
/var/lib/docker/overlay2/0ce3a0b1ec0fa5d2c7e300b004f1eecf1a28db187751c60e179c96c17ae3d5a6/merged/etc/nova/nova.conf
/var/lib/docker/overlay2/11acf3ad2d7b19883802b563803e0d7997fd94055015eced5aa35e1a33b97f67/diff/etc/nova/nova.conf
/var/lib/docker/overlay2/11acf3ad2d7b19883802b563803e0d7997fd94055015eced5aa35e1a33b97f67/merged/etc/nova/nova.conf











Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible. <type 'exceptions.ValueError'> (HTTP 500) (Request-ID: req-705b6e2d-45b4-4cf8-a752-6cae94a3e4f4)